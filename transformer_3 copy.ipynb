{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for feature: 8_x\n",
      "[10/30] train loss: 0.22, valid loss: 1.09\n",
      "[20/30] train loss: 0.17, valid loss: 0.19\n",
      "[30/30] train loss: 0.14, valid loss: 0.52\n",
      "Testing for feature: 8_x\n",
      "Training for feature: 8_y\n",
      "[10/30] train loss: 0.35, valid loss: 1.94\n",
      "[20/30] train loss: 0.30, valid loss: 0.69\n",
      "[30/30] train loss: 0.19, valid loss: 1.80\n",
      "Testing for feature: 8_y\n",
      "Training for feature: 8_z\n",
      "[10/30] train loss: 0.27, valid loss: 0.22\n",
      "[20/30] train loss: 0.20, valid loss: 0.28\n",
      "[30/30] train loss: 0.19, valid loss: 0.28\n",
      "Testing for feature: 8_z\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#【PyTorch】Transformerによる時系列予測\n",
    "# Library\n",
    "\n",
    "# ライブラリのインポート\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import LayerNorm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "\n",
    "# ランダムシードの設定\n",
    "fix_seed = 2023\n",
    "np.random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# データのロードと実験用の整形\n",
    "\n",
    "class AirPassengersDataset(Dataset):\n",
    "    def __init__(self, flag, seq_len, pred_len, feature):\n",
    "        # 学習期間と予測期間の設定\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.feature = feature  # 追加\n",
    "\n",
    "        # 訓練用、評価用、テスト用を分けるためのフラグ\n",
    "        type_map = {'train': 0, 'val': 1, 'test':2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        \n",
    "        # seabornのデータセットから飛行機の搭乗者数のデータをロード\n",
    "        df_raw = pd.read_csv('hand_300.csv')\n",
    "\n",
    "        # 訓練用、評価用、テスト用で呼び出すデータを変える\n",
    "        border1s = [0, 12 * 9 - self.seq_len, 12 * 11 - self.seq_len]\n",
    "        border2s = [12 * 9, 12 * 11, 12 * 12]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        data = df_raw[[self.feature]].values  # 修正\n",
    "        ss = StandardScaler()\n",
    "        data = ss.fit_transform(data)\n",
    "\n",
    "        self.data = data[border1:border2]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 学習用の系列と予測用の系列を出力\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end\n",
    "        r_end = r_begin + self.pred_len\n",
    "\n",
    "        src = self.data[s_begin:s_end]\n",
    "        tgt = self.data[r_begin:r_end]\n",
    "\n",
    "        return src, tgt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "# DataLoaderの定義\n",
    "\n",
    "def data_provider(flag, seq_len, pred_len, batch_size, feature):  # 追加\n",
    "    # flagに合ったデータを出力\n",
    "    data_set = AirPassengersDataset(flag=flag, \n",
    "                                    seq_len=seq_len, \n",
    "                                    pred_len=pred_len,\n",
    "                                    feature=feature  # 追加\n",
    "                                   )\n",
    "    # データをバッチごとに分けて出力できるDataLoaderを使用\n",
    "    data_loader = DataLoader(data_set,\n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True\n",
    "                            )\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "# エンべディングの定義\n",
    "\n",
    "# 位置エンコーディングの定義\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# モデルに入力するために次元を拡張する\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Linear(c_in, d_model) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x)\n",
    "        return x\n",
    "\n",
    "# Transformerの定義\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers,\n",
    "        d_model, d_input, d_output,\n",
    "        dim_feedforward = 512, dropout = 0.1, nhead = 8):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "\n",
    "        # エンべディングの定義\n",
    "        self.token_embedding_src = TokenEmbedding(d_input, d_model)\n",
    "        self.token_embedding_tgt = TokenEmbedding(d_output, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # エンコーダの定義\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, \n",
    "                                                nhead=nhead, \n",
    "                                                dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout,\n",
    "                                                batch_first=True,\n",
    "                                                activation='gelu'\n",
    "                                               )\n",
    "        encoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, \n",
    "                                                      num_layers=num_encoder_layers,\n",
    "                                                      norm=encoder_norm\n",
    "                                                     )\n",
    "        \n",
    "        # デコーダの定義\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=d_model, \n",
    "                                                nhead=nhead, \n",
    "                                                dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout,\n",
    "                                                batch_first=True,\n",
    "                                                activation='gelu'\n",
    "                                               )\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, \n",
    "                                                      num_layers=num_decoder_layers, \n",
    "                                                      norm=decoder_norm)\n",
    "        \n",
    "        # 出力層の定義\n",
    "        self.output = nn.Linear(d_model, d_output)\n",
    "        \n",
    "\n",
    "    def forward(self, src, tgt, mask_src, mask_tgt):\n",
    "        # mask_src, mask_tgtはセルフアテンションの際に未来のデータにアテンションを向けないためのマスク\n",
    "        \n",
    "        embedding_src = self.positional_encoding(self.token_embedding_src(src))\n",
    "        memory = self.transformer_encoder(embedding_src, mask_src)\n",
    "        \n",
    "        embedding_tgt = self.positional_encoding(self.token_embedding_tgt(tgt))\n",
    "        outs = self.transformer_decoder(embedding_tgt, memory, mask_tgt)\n",
    "        \n",
    "        output = self.output(outs)\n",
    "        return output\n",
    "\n",
    "    def encode(self, src, mask_src):\n",
    "        return self.transformer_encoder(self.positional_encoding(self.token_embedding_src(src)), mask_src)\n",
    "\n",
    "    def decode(self, tgt, memory, mask_tgt):\n",
    "        return self.transformer_decoder(self.positional_encoding(self.token_embedding_tgt(tgt)), memory, mask_tgt)\n",
    "\n",
    "# マスクの定義\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    \n",
    "    seq_len_src = src.shape[1]\n",
    "    seq_len_tgt = tgt.shape[1]\n",
    "\n",
    "    mask_tgt = generate_square_subsequent_mask(seq_len_tgt).to(device)\n",
    "    mask_src = generate_square_subsequent_mask(seq_len_src).to(device)\n",
    "\n",
    "    return mask_src, mask_tgt\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(seq_len):\n",
    "    mask = torch.triu(torch.full((seq_len, seq_len), float('-inf')), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "# 訓練、評価の処理を定義\n",
    "\n",
    "def train(model, data_provider, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    for src, tgt in data_provider:\n",
    "        \n",
    "        src = src.float().to(device)\n",
    "        tgt = tgt.float().to(device)\n",
    "\n",
    "        input_tgt = torch.cat((src[:,-1:,:],tgt[:,:-1,:]), dim=1)\n",
    "\n",
    "        mask_src, mask_tgt = create_mask(src, input_tgt)\n",
    "\n",
    "        output = model(\n",
    "            src=src, tgt=input_tgt, \n",
    "            mask_src=mask_src, mask_tgt=mask_tgt\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        total_loss.append(loss.cpu().detach())\n",
    "        optimizer.step()\n",
    "        \n",
    "    return np.average(total_loss)\n",
    "\n",
    "\n",
    "def evaluate(flag, model, data_provider, criterion):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    for src, tgt in data_provider:\n",
    "        \n",
    "        src = src.float().to(device)\n",
    "        tgt = tgt.float().to(device)\n",
    "\n",
    "        seq_len_src = src.shape[1]\n",
    "        mask_src = (torch.zeros(seq_len_src, seq_len_src)).type(torch.bool)\n",
    "        mask_src = mask_src.float().to(device)\n",
    "    \n",
    "        memory = model.encode(src, mask_src)\n",
    "        outputs = src[:, -1:, :]\n",
    "        seq_len_tgt = tgt.shape[1]\n",
    "    \n",
    "        for i in range(seq_len_tgt - 1):\n",
    "        \n",
    "            mask_tgt = (generate_square_subsequent_mask(outputs.size(1))).to(device)\n",
    "        \n",
    "            output = model.decode(outputs, memory, mask_tgt)\n",
    "            output = model.output(output)\n",
    "\n",
    "            outputs = torch.cat([outputs, output[:, -1:, :]], dim=1)\n",
    "        \n",
    "        loss = criterion(outputs, tgt)\n",
    "        total_loss.append(loss.cpu().detach())\n",
    "        \n",
    "    if flag=='test':\n",
    "        true = torch.cat((src, tgt), dim=1)\n",
    "        pred = torch.cat((src, output), dim=1)\n",
    "        plt.plot(true.squeeze().cpu().detach().numpy(), label='true')\n",
    "        plt.plot(pred.squeeze().cpu().detach().numpy(), label='pred')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'test_{flag}.pdf')  # 修正\n",
    "        \n",
    "    return np.average(total_loss)\n",
    "\n",
    "# パラメータなどの定義\n",
    "\n",
    "d_input = 1\n",
    "d_output = 1\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "dropout = 0.01\n",
    "src_len = 36\n",
    "tgt_len = 12\n",
    "batch_size = 1\n",
    "epochs = 30\n",
    "best_loss = float('Inf')\n",
    "best_model = None\n",
    "\n",
    "features = ['8_x', '8_y', '8_z']  # 追加\n",
    "\n",
    "for feature in features:\n",
    "    print(f\"Training for feature: {feature}\")\n",
    "    \n",
    "    model = Transformer(num_encoder_layers=num_encoder_layers,\n",
    "                        num_decoder_layers=num_decoder_layers,\n",
    "                        d_model=d_model,\n",
    "                        d_input=d_input, \n",
    "                        d_output=d_output,\n",
    "                        dim_feedforward=dim_feedforward,\n",
    "                        dropout=dropout, nhead=nhead\n",
    "                       )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.RAdam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # 訓練と評価用データにおける評価\n",
    "\n",
    "    valid_losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        loss_train = train(\n",
    "            model=model, data_provider=data_provider('train', src_len, tgt_len, batch_size, feature), optimizer=optimizer,\n",
    "            criterion=criterion\n",
    "        )\n",
    "            \n",
    "        loss_valid = evaluate(\n",
    "            flag='val', model=model, data_provider=data_provider('val', src_len, tgt_len, batch_size, feature), criterion=criterion\n",
    "        )\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print('[{}/{}] train loss: {:.2f}, valid loss: {:.2f}'.format(\n",
    "                epoch, epochs,\n",
    "                loss_train, loss_valid,\n",
    "            ))\n",
    "            \n",
    "        valid_losses.append(loss_valid)\n",
    "        \n",
    "        if best_loss > loss_valid:\n",
    "            best_loss = loss_valid\n",
    "            best_model = model\n",
    "\n",
    "    # テスト用データにおける予測と出力\n",
    "    print(f\"Testing for feature: {feature}\")\n",
    "    evaluate(flag='test', model=best_model, data_provider=data_provider('test', src_len, tgt_len, batch_size, feature), criterion=criterion)\n",
    "    plt.title(f'Test results for {feature}')\n",
    "    plt.savefig(f'test_{feature}.pdf')\n",
    "    plt.clf()  # グラフをクリアして次のプロットに備える\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer_TimeSeriesForecasting-main-LRDpDuFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
