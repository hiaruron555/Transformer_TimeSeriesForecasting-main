{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのトレーニングと評価を開始します\n",
      "torch.Size([24457, 63])\n",
      "torch.Size([24457, 63])\n",
      "ループ回数: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1055191/3094069842.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (24457) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 336\u001b[0m\n\u001b[1;32m    334\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    335\u001b[0m mask_x_train, mask_y_train \u001b[38;5;241m=\u001b[39m create_mask(x_train, y_train)\n\u001b[0;32m--> 336\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_x_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_y_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m#outputs = model(x_batch)\u001b[39;00m\n\u001b[1;32m    338\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Transformer_TimeSeriesForecasting-main-LRDpDuFD/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Transformer_TimeSeriesForecasting-main-LRDpDuFD/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 108\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x_train, y_train, mask_x_train, mask_y_train)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_train, y_train, mask_x_train, mask_y_train):\n\u001b[0;32m--> 108\u001b[0m     embedding_x_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding_x_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(embedding_x_train, mask_x_train)\n\u001b[1;32m    110\u001b[0m     embedding_y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_y_train(y_train))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Transformer_TimeSeriesForecasting-main-LRDpDuFD/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Transformer_TimeSeriesForecasting-main-LRDpDuFD/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 79\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     78\u001b[0m     pe_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe[:, :x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), :]  \u001b[38;5;66;03m# xの長さに合わせて位置エンコーディングを切り取る\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpe_slice\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (24457) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import LayerNorm\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# フラットな手の座標をMediaPipe Handランドマークに変換する関数\n",
    "def flatten_to_landmarks(coordinates):\n",
    "    landmarks = []\n",
    "    for i in range(0, len(coordinates), 3):\n",
    "        landmarks.append((coordinates[i], coordinates[i + 1], coordinates[i + 2]))\n",
    "    return landmarks\n",
    "\n",
    "# データの読み込みと前処理\n",
    "def preprocess_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for i in range(len(df) - n_seq):\n",
    "        x_sequence = df.iloc[i:i+n_seq][[f'{j}_{c}' for j in range(num_joints) for c in ['x', 'y', 'z']]].values\n",
    "        x_data.append(x_sequence)\n",
    "\n",
    "        y_sequence = df.iloc[i+n_seq][[f'{j}_{c}' for j in range(num_joints) for c in ['x', 'y', 'z']]].values\n",
    "        y_data.append(y_sequence)\n",
    "\n",
    "    x_data = np.array(x_data, dtype=np.float32)\n",
    "    y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "# じゃんけんの手のラベル\n",
    "janken_labels = {0: 'チョキ', 1: 'グー', 2: 'パー'}\n",
    "\n",
    "# DataLoaderの使用\n",
    "def create_dataloader(x_train, y_train, batch_size):\n",
    "    train_dataset = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "# 位置エンコーディングの定義\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe_slice = self.pe[:, :x.size(1), :]  # xの長さに合わせて位置エンコーディングを切り取る\n",
    "        x = x + pe_slice\n",
    "        return self.dropout(x)\n",
    "\n",
    "# モデルに入力するために次元を拡張する\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Linear(c_in, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x)\n",
    "        return x\n",
    "\n",
    "# Transformerの定義\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, d_model, d_input, d_output, dim_feedforward=512, dropout=0.1, nhead=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.token_embedding_x_train = TokenEmbedding(d_input, d_model)\n",
    "        self.token_embedding_y_train = TokenEmbedding(d_output, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True, activation='gelu')\n",
    "        encoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers, norm=encoder_norm)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True, activation='gelu')\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers, norm=decoder_norm)\n",
    "        self.output = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x_train, y_train, mask_x_train, mask_y_train):\n",
    "        embedding_x_train = self.positional_encoding(self.token_embedding_x_train(x_train))\n",
    "        memory = self.transformer_encoder(embedding_x_train, mask_x_train)\n",
    "        embedding_y_train = self.positional_encoding(self.token_embedding_y_train(y_train))\n",
    "        outs = self.transformer_decoder(embedding_y_train, memory, mask_y_train)\n",
    "        output = self.output(outs)\n",
    "        return output\n",
    "\n",
    "    def encode(self, x_train, mask_x_train):\n",
    "        return self.transformer_encoder(self.positional_encoding(self.token_embedding_x_train(x_train)), mask_x_train)\n",
    "\n",
    "    def decode(self, y_train, memory, mask_y_train):\n",
    "        return self.transformer_decoder(self.positional_encoding(self.token_embedding_y_train(y_train)), memory, mask_y_train)\n",
    "\n",
    "def expand_dims_if_needed(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        if data.ndim == 2:\n",
    "            data = np.expand_dims(data, axis=1)\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        if data.dim() == 2:\n",
    "            data = data.unsqueeze(1)\n",
    "    return data\n",
    "\n",
    "\n",
    "# データが numpy.ndarray または torch.Tensor の場合に対応する関数\n",
    "def squeeze_dims_if_needed(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        if data.ndim == 3 and data.shape[1] == 1:\n",
    "            data = np.squeeze(data, axis=1)\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        if data.dim() == 3 and data.size(1) == 1:\n",
    "            data = data.squeeze(1)\n",
    "    return data\n",
    "\n",
    "# numpy.ndarray を torch.Tensor に変換する関数\n",
    "def to_tensor(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = torch.tensor(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# マスクの定義\n",
    "def create_mask(x_train, y_train):\n",
    "    seq_len_x_train = x_train.shape[1]\n",
    "    seq_len_y_train = y_train.shape[1]\n",
    "    mask_y_train = generate_square_subsequent_mask(seq_len_y_train).to(device)\n",
    "    mask_x_train = generate_square_subsequent_mask(seq_len_x_train).to(device)\n",
    "    return mask_x_train, mask_y_train\n",
    "\n",
    "def generate_square_subsequent_mask(size):\n",
    "    \"\"\"生成したマスクは、将来の時刻のトークンをマスクするための上三角行列です。\"\"\"\n",
    "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\"\"\"\"\n",
    "# 訓練、評価の処理を定義\n",
    "def train(model, data_provider, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    for x_train, y_train in data_provider:\n",
    "        x_train = x_train.float().to(device)\n",
    "        y_train = y_train.float().to(device)\n",
    "        \n",
    "        # 次元のチェックと拡張\n",
    "        if x_train.dim() == 2:\n",
    "            x_train = x_train.unsqueeze(1)\n",
    "        if y_train.dim() == 2:\n",
    "            y_train = y_train.unsqueeze(1)\n",
    "        \n",
    "        input_y_train = torch.cat((x_train[:, -1:, :], y_train[:, :-1, :]), dim=1)\n",
    "        mask_x_train, mask_y_train = create_mask(x_train, input_y_train)\n",
    "        output = model(x_train=x_train, y_train=input_y_train, mask_x_train=mask_x_train, mask_y_train=mask_y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        total_loss.append(loss.cpu().detach())\n",
    "        optimizer.step()\n",
    "    return np.average(total_loss)\n",
    "\n",
    "def evaluate(flag, model, data_provider, criterion):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for x_train, y_train in data_provider:\n",
    "        x_train = x_train.float().to(device)\n",
    "        y_train = y_train.float().to(device)\n",
    "        \n",
    "        # 次元のチェックと拡張\n",
    "        if x_train.dim() == 2:\n",
    "            x_train = x_train.unsqueeze(1)\n",
    "        if y_train.dim() == 2:\n",
    "            y_train = y_train.unsqueeze(1)\n",
    "\n",
    "        seq_len_x_train = x_train.shape[1]\n",
    "        mask_x_train = (torch.zeros(seq_len_x_train, seq_len_x_train)).type(torch.bool)\n",
    "        mask_x_train = mask_x_train.float().to(device)\n",
    "        memory = model.encode(x_train, mask_x_train)\n",
    "        outputs = x_train[:, -1:, :]\n",
    "        seq_len_y_train = y_train.shape[1]\n",
    "        for i in range(seq_len_y_train - 1):\n",
    "            mask_y_train = (generate_square_subsequent_mask(outputs.size(1))).to(device)\n",
    "            output = model.decode(outputs, memory, mask_y_train)\n",
    "            output = model.output(output)\n",
    "            outputs = torch.cat([outputs, output[:, -1:, :]], dim=1)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        total_loss.append(loss.cpu().detach())\n",
    "        all_true.append(torch.cat((x_train, y_train), dim=1).cpu().detach().numpy())\n",
    "        all_pred.append(torch.cat((x_train, outputs), dim=1).cpu().detach().numpy())\n",
    "    if flag == 'test':\n",
    "        true = np.concatenate(all_true)\n",
    "        pred = np.concatenate(all_pred)\n",
    "        df_true = pd.DataFrame(true.reshape(-1, 3), columns=['8_x', '8_y', '8_z'])\n",
    "        df_pred = pd.DataFrame(pred.reshape(-1, 3), columns=['8_x', '8_y', '8_z'])\n",
    "        df_true.to_csv('true_coordinates.csv', index=False)\n",
    "        df_pred.to_csv('predicted_coordinates.csv', index=False)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df_true['8_x'], label='true_x')\n",
    "        plt.plot(df_pred['8_x'], label='pred_x')\n",
    "        plt.plot(df_true['8_y'], label='true_y')\n",
    "        plt.plot(df_pred['8_y'], label='pred_y')\n",
    "        plt.plot(df_true['8_z'], label='true_z')\n",
    "        plt.plot(df_pred['8_z'], label='pred_z')\n",
    "        plt.legend()\n",
    "        plt.savefig('test_coordinates.pdf')\n",
    "    return np.average(total_loss)\n",
    "\"\"\"\n",
    "# パラメータなどの定義\n",
    "d_input = 3\n",
    "d_output = 3\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "dropout = 0.01\n",
    "x_train_len = 36\n",
    "y_train_len = 12\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "best_loss = float('Inf')\n",
    "best_model = None\n",
    "\n",
    "n_seq = 1\n",
    "num_joints = 21\n",
    "input_size = num_joints * 3\n",
    "hidden_size = 16\n",
    "output_size = num_joints * 3\n",
    "num_layers = 1\n",
    "n_epochs = 1\n",
    "\n",
    "print(\"モデルのトレーニングと評価を開始します\")\n",
    "\n",
    "train_csv_path = 'hand_300.csv'\n",
    "test_csv_path = 'test_10/choki_test_10/choki_test.csv'\n",
    "\n",
    "#x_train, y_train = preprocess_data(train_csv_path)\n",
    "x_train, y_train = preprocess_data(train_csv_path)\n",
    "#x_test, y_test = preprocess_data(test_csv_path)\n",
    "x_test, y_test = preprocess_data(test_csv_path)\n",
    "\n",
    "x_train = expand_dims_if_needed(x_train)\n",
    "y_train = expand_dims_if_needed(y_train)\n",
    "\n",
    "# x_train, y_train が numpy.ndarray または torch.Tensor であることを想定\n",
    "x_train = squeeze_dims_if_needed(x_train)\n",
    "y_train = squeeze_dims_if_needed(y_train)\n",
    "\n",
    "# numpy.ndarray を torch.Tensor に変換\n",
    "x_train = to_tensor(x_train)\n",
    "y_train = to_tensor(y_train)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "#train_loader = create_dataloader(x_train, y_train)\n",
    "train_loader = create_dataloader(x_train, y_train, batch_size)\n",
    "\n",
    "model = Transformer(num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, d_model=d_model, d_input=num_joints * 3, d_output=num_joints * 3, dim_feedforward=dim_feedforward, dropout=dropout, nhead=nhead)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)\n",
    "#criterion = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.RAdam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "# 学習率のスケジューリング\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    print(\"ループ回数:\", i+1)\n",
    "    \n",
    "    # トレーニングループ\n",
    "    start_time = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.unsqueeze(1).to(device), y_batch.to(device)\n",
    "            \"\"\"\"\n",
    "            # 次元のチェックと拡張\n",
    "            if x_train.dim() == 2:\n",
    "                x_train = x_train.unsqueeze(1)\n",
    "            if y_train.dim() == 2:\n",
    "                y_train = y_train.unsqueeze(1)\n",
    "            \"\"\"\n",
    "            #input_y_train = torch.cat((x_train[:, -1:, :], y_train[:, :-1, :]), dim=1)\n",
    "            #input_y_train = torch.cat((x_train[:, -1:], y_train[:, :-1]), dim=1)\n",
    "            #mask_x_train, mask_y_train = create_mask(x_train, y_train)\n",
    "            #output = model(x_train=x_train, y_train=input_y_train, mask_x_train=mask_x_train, mask_y_train=mask_y_train)\n",
    "            #print(x_train)\n",
    "            #print(y_train)\n",
    "            #output = model(x_train=x_train, y_train=y_train, mask_x_train=mask_x_train, mask_y_train=mask_y_train)\n",
    "            #outputs = model(x_train, y_train, mask_x_train, mask_y_train)\n",
    "            #optimizer.zero_grad()\n",
    "            #loss = criterion(output, y_train)\n",
    "            #loss.backward()\n",
    "            #total_loss.append(loss.cpu().detach())\n",
    "            #optimizer.step()\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            mask_x_train, mask_y_train = create_mask(x_train, y_train)\n",
    "            outputs = model(x_train, y_train, mask_x_train, mask_y_train)\n",
    "            #outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    #print(f'学習時間: {training_time:.2f} 秒')\n",
    "\n",
    "    # テストデータで評価\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "\n",
    "        test_outputs = model(torch.tensor(x_test, dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        test_loss = criterion(test_outputs, torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "        \n",
    "        total_loss = []\n",
    "        all_true = []\n",
    "        all_pred = []\n",
    "        for x_train, y_train in data_provider:\n",
    "            x_train = x_train.float().to(device)\n",
    "            y_train = y_train.float().to(device)\n",
    "        \n",
    "         # 次元のチェックと拡張\n",
    "            if x_train.dim() == 2:\n",
    "                x_train = x_train.unsqueeze(1)\n",
    "            if y_train.dim() == 2:\n",
    "                y_train = y_train.unsqueeze(1)\n",
    "\n",
    "            seq_len_x_train = x_train.shape[1]\n",
    "            mask_x_train = (torch.zeros(seq_len_x_train, seq_len_x_train)).type(torch.bool)\n",
    "            mask_x_train = mask_x_train.float().to(device)\n",
    "            memory = model.encode(x_train, mask_x_train)\n",
    "            outputs = x_train[:, -1:, :]\n",
    "            seq_len_y_train = y_train.shape[1]\n",
    "            for i in range(seq_len_y_train - 1):\n",
    "                mask_y_train = (generate_square_subsequent_mask(outputs.size(1))).to(device)\n",
    "                output = model.decode(outputs, memory, mask_y_train)\n",
    "                output = model.output(output)\n",
    "                outputs = torch.cat([outputs, output[:, -1:, :]], dim=1)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            total_loss.append(loss.cpu().detach())\n",
    "            all_true.append(torch.cat((x_train, y_train), dim=1).cpu().detach().numpy())\n",
    "            all_pred.append(torch.cat((x_train, outputs), dim=1).cpu().detach().numpy())\n",
    " \n",
    "        processing_time_per_image = time.time() - start_time\n",
    "        #print(f'Test Loss: {test_loss.item():.4f}')\n",
    "        #print(f'処理時間_1: {processing_time_per_image:.6f} 秒')\n",
    "\n",
    "\n",
    "    # 3つのテストサンプルごとに予測結果を処理し、1つずつずらして繰り返す\n",
    "    window_size = 3  # ウィンドウサイズ（処理するテストサンプルの数）\n",
    "\n",
    "    for sample_index in range(0, len(x_test)):  # ウィンドウを1つずつずらして処理する\n",
    "    #for sample_index in range(0, len(x_test) - window_size + 1):  # ウィンドウを1つずつずらして処理する\n",
    "        # ウィンドウ内のテストサンプルを取得\n",
    "        x_test_batch = torch.tensor(x_test[sample_index:sample_index+window_size], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        y_test_batch = torch.tensor(y_test[sample_index:sample_index+window_size], dtype=torch.float32).to(device)\n",
    "        print(sample_index+1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 既存のモデルでx_testを予測\n",
    "            start_time = time.time()\n",
    "            predicted_tensor_x = model(x_test_batch)\n",
    "            predicted_x = predicted_tensor_x.cpu().numpy()\n",
    "            processing_time_per_image = time.time() - start_time\n",
    "            #print(f'処理時間_2: {processing_time_per_image:.6f} 秒')\n",
    "\n",
    "            # 新しいモデルのインスタンスを作成\n",
    "            #new_model = Net()\n",
    "            #new_model.load_state_dict(torch.load('hanbetu_all.pth'))\n",
    "            #new_model.to(device)\n",
    "            #new_model.eval()\n",
    "\n",
    "            # 既存の手の座標を指定\n",
    "            # 予測結果の後に続く処理\n",
    "            ground_truth_landmarks = flatten_to_landmarks(y_test_batch[0])  # 修正\n",
    "            #ground_truth_landmarks = flatten_to_landmarks(y_test[sample_index])\n",
    "            truth_landmarks = flatten_to_landmarks(x_test[sample_index])\n",
    "            sample_landmarks_x = flatten_to_landmarks(predicted_x[0])  # 予測結果を利用\n",
    "\n",
    "            # landmark_0 の x 座標と y 座標の差分を計算し、表示する\n",
    "            x_difference = y_test[sample_index][0] - predicted_x[0][0]\n",
    "            y_difference = y_test[sample_index][1] - predicted_x[0][1]\n",
    "            #print(f'Landmark 0 の x 座標の差分: {x_difference}')\n",
    "            #print(f'Landmark 0 の y 座標の差分: {y_difference}')\n",
    "            \n",
    "            # sample_landmarks_xに含まれるすべてのデータを修正してリストに変換\n",
    "            corrected = []\n",
    "            for landmark in sample_landmarks_x:\n",
    "                x_corrected = landmark[0] + x_difference\n",
    "                y_corrected = landmark[1] + y_difference\n",
    "                z_corrected = landmark[2]  # z座標は変更しないと仮定\n",
    "                corrected.append((x_corrected, y_corrected, z_corrected))\n",
    "    \n",
    "            # リストに変換\n",
    "            corrected = [list(landmark) for landmark in corrected]\n",
    "            #print(\"corrected:\", corrected)\n",
    "\n",
    "\n",
    "            # ground_truth_landmarksとsample_landmarks_xをTensorに変換\n",
    "            ground_truth_tensor = torch.tensor(ground_truth_landmarks, dtype=torch.float32)\n",
    "            sample_landmarks_x_tensor = torch.tensor(sample_landmarks_x, dtype=torch.float32)\n",
    "            # MSELossを計算\n",
    "            loss = criterion(ground_truth_tensor, sample_landmarks_x_tensor)\n",
    "            #print(f'MSE Loss(sample): {loss.item():.4f}')\n",
    "\n",
    "            \n",
    "            # ground_truth_landmarksとcorrectedをTensorに変換\n",
    "            ground_truth_tensor = torch.tensor(ground_truth_landmarks, dtype=torch.float32)\n",
    "            corrected_tensor = torch.tensor(corrected, dtype=torch.float32)\n",
    "            # MSELossを計算\n",
    "            loss = criterion(ground_truth_tensor, corrected_tensor)\n",
    "            print(f'MSE Loss(corrected): {loss.item():.4f}')\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "            # プロットの設定\n",
    "            save_path = f'/home/iwata/Pictures/hand_{sample_index}_n{n_seq}_r{i+1}_w{window_size}_{n_epochs}.png'\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            #ax.set_aspect('equal', 'box')\n",
    "            #print(\"ground_truth_landmarks:\", ground_truth_landmarks)\n",
    "            #print(\"sample_landmarks_x:\",sample_landmarks_x)\n",
    "\n",
    "            # 手の座標点の順序を指定するリスト（例）\n",
    "            custom_order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "            # Ground Truthの手の線をプロット\n",
    "            for points in [[0, 1, 2, 3, 4], [0, 5, 6, 7, 8], [5, 9, 10, 11, 12], [9, 13, 14, 15, 16], [13, 17, 18, 19, 20], [0, 17]]:\n",
    "                x_points = [ground_truth_landmarks[i][0] for i in points]\n",
    "                y_points = [ground_truth_landmarks[i][1] for i in points]\n",
    "                ax.plot(x_points, y_points, linestyle='-', color='blue', linewidth=2)\n",
    "               \n",
    "            # correctedの手の線をプロット\n",
    "            for points in [[0, 1, 2, 3, 4], [0, 5, 6, 7, 8], [5, 9, 10, 11, 12], [9, 13, 14, 15, 16], [13, 17, 18, 19, 20], [0, 17]]:\n",
    "                x_points = [corrected[i][0] for i in points]\n",
    "                y_points = [corrected[i][1] for i in points]\n",
    "                ax.plot(x_points, y_points, linestyle='-', color='red', linewidth=2)\n",
    "                    \n",
    "            # truth_landmarksの手の線をプロット\n",
    "            for points in [[0, 1, 2, 3, 4], [0, 5, 6, 7, 8], [5, 9, 10, 11, 12], [9, 13, 14, 15, 16], [13, 17, 18, 19, 20], [0, 17]]:\n",
    "                x_points = [truth_landmarks[i][0] for i in points]\n",
    "                y_points = [truth_landmarks[i][1] for i in points]\n",
    "                ax.plot(x_points, y_points, linestyle='-', color='green', linewidth=2)\n",
    "                \n",
    "        \n",
    "            #plt.legend()\n",
    "\n",
    "            # 画像を保存\n",
    "            plt.savefig(save_path)\n",
    "\n",
    "            # 画像を表示\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer_TimeSeriesForecasting-main-LRDpDuFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
