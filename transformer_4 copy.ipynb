{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Training and evaluating model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_provider' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 330\u001b[0m\n\u001b[1;32m    326\u001b[0m valid_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    329\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m--> 330\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, data_provider\u001b[38;5;241m=\u001b[39m\u001b[43mdata_provider\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, src_len, tgt_len, batch_size), optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m    331\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    334\u001b[0m     loss_valid \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m    335\u001b[0m         flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, data_provider\u001b[38;5;241m=\u001b[39mdata_provider(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, src_len, tgt_len, batch_size), criterion\u001b[38;5;241m=\u001b[39mcriterion\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_provider' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import LayerNorm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# フラットな手の座標をMediaPipe Handランドマークに変換する関数\n",
    "def flatten_to_landmarks(coordinates):\n",
    "    landmarks = []\n",
    "    for i in range(0, len(coordinates), 3):\n",
    "        landmarks.append((coordinates[i], coordinates[i + 1], coordinates[i + 2]))\n",
    "    return landmarks\n",
    "\n",
    "# データの読み込みと前処理\n",
    "def preprocess_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for i in range(len(df) - n_seq):\n",
    "        x_sequence = df.iloc[i][[f'{j}_{c}' for j in range(num_joints) for c in ['x', 'y', 'z']]].values.flatten() \n",
    "        x_data.append(x_sequence)\n",
    "        \n",
    "        y_sequence = df.iloc[i+n_seq][[f'{j}_{c}' for j in range(num_joints) for c in ['x', 'y', 'z']]].values.flatten()\n",
    "        y_data.append(y_sequence)\n",
    "\n",
    "    x_data = np.array(x_data, dtype=np.float32)  # float32に変換\n",
    "    y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "# じゃんけんの手のラベル\n",
    "janken_labels = {0: 'チョキ', 1: 'グー', 2: 'パー'}\n",
    "\n",
    "\n",
    "# DataLoaderの使用\n",
    "def create_dataloader(x_train, y_train):\n",
    "    train_dataset = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "\n",
    "class AirPassengersDataset(Dataset):\n",
    "    def __init__(self, flag, seq_len, pred_len, feature):\n",
    "        # 学習期間と予測期間の設定\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.feature = feature  # 追加\n",
    "\n",
    "        # 訓練用、評価用、テスト用を分けるためのフラグ\n",
    "        type_map = {'train': 0, 'val': 1, 'test':2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        \n",
    "        # seabornのデータセットから飛行機の搭乗者数のデータをロード\n",
    "        df_raw = pd.read_csv('hand_300.csv')\n",
    "\n",
    "        # 訓練用、評価用、テスト用で呼び出すデータを変える\n",
    "        border1s = [0, 12 * 9 - self.seq_len, 12 * 11 - self.seq_len]\n",
    "        border2s = [12 * 9, 12 * 11, 12 * 12]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        data = df_raw[[self.feature]].values  # 修正\n",
    "        ss = StandardScaler()\n",
    "        data = ss.fit_transform(data)\n",
    "\n",
    "        self.data = data[border1:border2]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 学習用の系列と予測用の系列を出力\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end\n",
    "        r_end = r_begin + self.pred_len\n",
    "\n",
    "        src = self.data[s_begin:s_end]\n",
    "        tgt = self.data[r_begin:r_end]\n",
    "\n",
    "        return src, tgt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "# DataLoaderの定義\n",
    "\n",
    "def data_provider(flag, seq_len, pred_len, batch_size, feature):  # 追加\n",
    "    # flagに合ったデータを出力\n",
    "    data_set = AirPassengersDataset(flag=flag, \n",
    "                                    seq_len=seq_len, \n",
    "                                    pred_len=pred_len,\n",
    "                                    feature=feature  # 追加\n",
    "                                   )\n",
    "    # データをバッチごとに分けて出力できるDataLoaderを使用\n",
    "    data_loader = DataLoader(data_set,\n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True\n",
    "                            )\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 位置エンコーディングの定義\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# モデルに入力するために次元を拡張する\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Linear(c_in, d_model) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x)\n",
    "        return x\n",
    "\n",
    "# Transformerの定義\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers,\n",
    "        d_model, d_input, d_output,\n",
    "        dim_feedforward = 512, dropout = 0.1, nhead = 8):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # エンべディングの定義\n",
    "        self.token_embedding_src = TokenEmbedding(d_input, d_model)\n",
    "        self.token_embedding_tgt = TokenEmbedding(d_output, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # エンコーダの定義\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, \n",
    "                                                nhead=nhead, \n",
    "                                                dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout,\n",
    "                                                batch_first=True,\n",
    "                                                activation='gelu'\n",
    "                                               )\n",
    "        encoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, \n",
    "                                                      num_layers=num_encoder_layers,\n",
    "                                                      norm=encoder_norm\n",
    "                                                     )\n",
    "        \n",
    "        # デコーダの定義\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=d_model, \n",
    "                                                nhead=nhead, \n",
    "                                                dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout,\n",
    "                                                batch_first=True,\n",
    "                                                activation='gelu'\n",
    "                                               )\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, \n",
    "                                                      num_layers=num_decoder_layers, \n",
    "                                                      norm=decoder_norm)\n",
    "        \n",
    "        # 出力層の定義\n",
    "        self.output = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, src, tgt, mask_src, mask_tgt):\n",
    "        # mask_src, mask_tgtはセルフアテンションの際に未来のデータにアテンションを向けないためのマスク\n",
    "        \n",
    "        embedding_src = self.positional_encoding(self.token_embedding_src(src))\n",
    "        memory = self.transformer_encoder(embedding_src, mask_src)\n",
    "        \n",
    "        embedding_tgt = self.positional_encoding(self.token_embedding_tgt(tgt))\n",
    "        outs = self.transformer_decoder(embedding_tgt, memory, mask_tgt)\n",
    "        \n",
    "        output = self.output(outs)\n",
    "        return output\n",
    "\n",
    "    def encode(self, src, mask_src):\n",
    "        return self.transformer_encoder(self.positional_encoding(self.token_embedding_src(src)), mask_src)\n",
    "\n",
    "    def decode(self, tgt, memory, mask_tgt):\n",
    "        return self.transformer_decoder(self.positional_encoding(self.token_embedding_tgt(tgt)), memory, mask_tgt)\n",
    "\n",
    "# マスクの定義\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    \n",
    "    seq_len_src = src.shape[1]\n",
    "    seq_len_tgt = tgt.shape[1]\n",
    "\n",
    "    mask_tgt = generate_square_subsequent_mask(seq_len_tgt).to(device)\n",
    "    mask_src = generate_square_subsequent_mask(seq_len_src).to(device)\n",
    "\n",
    "    return mask_src, mask_tgt\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(seq_len):\n",
    "    mask = torch.triu(torch.full((seq_len, seq_len), float('-inf')), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "# 訓練、評価の処理を定義\n",
    "\n",
    "def train(model, data_provider, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    for src, tgt in data_provider:\n",
    "        \n",
    "        src = src.float().to(device)\n",
    "        tgt = tgt.float().to(device)\n",
    "\n",
    "        input_tgt = torch.cat((src[:,-1:,:],tgt[:,:-1,:]), dim=1)\n",
    "\n",
    "        mask_src, mask_tgt = create_mask(src, input_tgt)\n",
    "\n",
    "        output = model(\n",
    "            src=src, tgt=input_tgt, \n",
    "            mask_src=mask_src, mask_tgt=mask_tgt\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        total_loss.append(loss.cpu().detach())\n",
    "        optimizer.step()\n",
    "        \n",
    "    return np.average(total_loss)\n",
    "\n",
    "\n",
    "def evaluate(flag, model, data_provider, criterion):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    \n",
    "    for src, tgt in data_provider:\n",
    "        \n",
    "        src = src.float().to(device)\n",
    "        tgt = tgt.float().to(device)\n",
    "\n",
    "        seq_len_src = src.shape[1]\n",
    "        mask_src = (torch.zeros(seq_len_src, seq_len_src)).type(torch.bool)\n",
    "        mask_src = mask_src.float().to(device)\n",
    "    \n",
    "        memory = model.encode(src, mask_src)\n",
    "        outputs = src[:, -1:, :]\n",
    "        seq_len_tgt = tgt.shape[1]\n",
    "    \n",
    "        for i in range(seq_len_tgt - 1):\n",
    "        \n",
    "            mask_tgt = (generate_square_subsequent_mask(outputs.size(1))).to(device)\n",
    "        \n",
    "            output = model.decode(outputs, memory, mask_tgt)\n",
    "            output = model.output(output)\n",
    "\n",
    "            outputs = torch.cat([outputs, output[:, -1:, :]], dim=1)\n",
    "        \n",
    "        loss = criterion(outputs, tgt)\n",
    "        total_loss.append(loss.cpu().detach())\n",
    "        \n",
    "        all_true.append(torch.cat((src, tgt), dim=1).cpu().detach().numpy())\n",
    "        all_pred.append(torch.cat((src, outputs), dim=1).cpu().detach().numpy())\n",
    "        \n",
    "    if flag == 'test':\n",
    "        true = np.concatenate(all_true)\n",
    "        pred = np.concatenate(all_pred)\n",
    "        \n",
    "        # 表に出力\n",
    "        df_true = pd.DataFrame(true.reshape(-1, 3), columns=['8_x', '8_y', '8_z'])\n",
    "        df_pred = pd.DataFrame(pred.reshape(-1, 3), columns=['8_x', '8_y', '8_z'])\n",
    "        \n",
    "        df_true.to_csv('true_coordinates.csv', index=False)\n",
    "        df_pred.to_csv('predicted_coordinates.csv', index=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df_true['8_x'], label='true_x')\n",
    "        plt.plot(df_pred['8_x'], label='pred_x')\n",
    "        plt.plot(df_true['8_y'], label='true_y')\n",
    "        plt.plot(df_pred['8_y'], label='pred_y')\n",
    "        plt.plot(df_true['8_z'], label='true_z')\n",
    "        plt.plot(df_pred['8_z'], label='pred_z')\n",
    "        plt.legend()\n",
    "        plt.savefig('test_coordinates.pdf')\n",
    "        \n",
    "    return np.average(total_loss)\n",
    "\n",
    "# パラメータなどの定義\n",
    "\n",
    "d_input = 3\n",
    "d_output = 3\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "dropout = 0.01\n",
    "src_len = 36\n",
    "tgt_len = 12\n",
    "batch_size = 1\n",
    "epochs = 30\n",
    "best_loss = float('Inf')\n",
    "best_model = None\n",
    "\n",
    "n_seq = 3\n",
    "print(n_seq)\n",
    "num_joints = 21\n",
    "input_size = num_joints * 3  # num_joints は前のコードで定義されているものと仮定\n",
    "hidden_size = 16\n",
    "output_size = num_joints * 3\n",
    "num_layers = 1\n",
    "batch_size = 36\n",
    "n_epochs = 100\n",
    "\n",
    "print(\"Training and evaluating model\")\n",
    "\n",
    "train_csv_path = 'hand_300.csv'\n",
    "test_csv_path = 'test_10/choki_test_10/choki_test.csv'\n",
    "\n",
    "x_train, y_train = preprocess_data(train_csv_path)\n",
    "x_test, y_test = preprocess_data(test_csv_path)\n",
    "\n",
    "#print(x_test)\n",
    "# DataLoaderの作成\n",
    "train_loader = create_dataloader(x_train, y_train)\n",
    "\n",
    "\n",
    "model = Transformer(num_encoder_layers=num_encoder_layers,\n",
    "                    num_decoder_layers=num_decoder_layers,\n",
    "                    d_model=d_model,\n",
    "                    d_input=d_input, \n",
    "                    d_output=d_output,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    dropout=dropout, nhead=nhead\n",
    "                   )\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 訓練と評価用データにおける評価\n",
    "\n",
    "valid_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    loss_train = train(\n",
    "        model=model, data_provider=data_provider('train', src_len, tgt_len, batch_size), optimizer=optimizer,\n",
    "        criterion=criterion\n",
    "    )\n",
    "        \n",
    "    loss_valid = evaluate(\n",
    "        flag='val', model=model, data_provider=data_provider('val', src_len, tgt_len, batch_size), criterion=criterion\n",
    "    )\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('[{}/{}] train loss: {:.2f}, valid loss: {:.2f}'.format(\n",
    "            epoch, epochs,\n",
    "            loss_train, loss_valid,\n",
    "        ))\n",
    "        \n",
    "    valid_losses.append(loss_valid)\n",
    "    \n",
    "    if best_loss > loss_valid:\n",
    "        best_loss = loss_valid\n",
    "        best_model = model\n",
    "\n",
    "# テスト用データにおける予測と出力\n",
    "\n",
    "print(\"Testing model\")\n",
    "evaluate(flag='test', model=best_model, data_provider=data_provider('test', src_len, tgt_len, batch_size), criterion=criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer_TimeSeriesForecasting-main-LRDpDuFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
