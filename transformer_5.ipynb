{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのトレーニングと評価を開始します\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xx_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 237\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    236\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m train(model\u001b[38;5;241m=\u001b[39mmodel, data_provider\u001b[38;5;241m=\u001b[39mtrain_loader, optimizer\u001b[38;5;241m=\u001b[39moptimizer, criterion\u001b[38;5;241m=\u001b[39mcriterion)\n\u001b[0;32m--> 237\u001b[0m     loss_valid \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] train loss: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m, valid loss: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, epochs, loss_train, loss_valid))\n",
      "Cell \u001b[0;32mIn[22], line 159\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(flag, model, data_provider, criterion)\u001b[0m\n\u001b[1;32m    157\u001b[0m mask_x_train \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mzeros(seq_len_x_train, seq_len_x_train))\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m    158\u001b[0m mask_x_train \u001b[38;5;241m=\u001b[39m mask_x_train\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 159\u001b[0m memory \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\u001b[43mxx_train\u001b[49m, mask_x_train)\n\u001b[1;32m    160\u001b[0m outputs \u001b[38;5;241m=\u001b[39m x_train[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]\n\u001b[1;32m    161\u001b[0m seq_len_y_data \u001b[38;5;241m=\u001b[39m y_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xx_train' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import LayerNorm\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import torch.optim as optim\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# フラットな手の座標をMediaPipe Handランドマークに変換する関数\n",
    "def flatten_to_landmarks(coordinates):\n",
    "    landmarks = []\n",
    "    for i in range(0, len(coordinates), 3):\n",
    "        landmarks.append((coordinates[i], coordinates[i + 1], coordinates[i + 2]))\n",
    "    return landmarks\n",
    "\n",
    "# データの読み込みと前処理\n",
    "def preprocess_data(csv_path, n_seq, num_joints):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for i in range(len(df) - n_seq):\n",
    "        x_sequence = df.iloc[i:i+n_seq][[f'{j}_{c}' for j in range(num_joints) for c in ['x', 'y', 'z']]].values\n",
    "        x_data.append(x_sequence)\n",
    "\n",
    "        y_sequence = df.iloc[i+n_seq][[f'{j}_{c}' for j in range(num_joints) for c in ['x', 'y', 'z']]].values\n",
    "        y_data.append(y_sequence)\n",
    "\n",
    "    x_data = np.array(x_data, dtype=np.float32)\n",
    "    y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "# じゃんけんの手のラベル\n",
    "janken_labels = {0: 'チョキ', 1: 'グー', 2: 'パー'}\n",
    "\n",
    "# DataLoaderの使用\n",
    "def create_dataloader(x_train, y_train, batch_size):\n",
    "    train_dataset = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "# 位置エンコーディングの定義\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# モデルに入力するために次元を拡張する\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Linear(c_in, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x)\n",
    "        return x\n",
    "\n",
    "# Transformerの定義\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, d_model, d_input, d_output, dim_feedforward=512, dropout=0.1, nhead=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.token_embedding_x_train = TokenEmbedding(d_input, d_model)\n",
    "        self.token_embedding_y_data = TokenEmbedding(d_output, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True, activation='gelu')\n",
    "        encoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers, norm=encoder_norm)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True, activation='gelu')\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers, norm=decoder_norm)\n",
    "        self.output = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x_train, y_data, mask_x_train, mask_y_data):\n",
    "        embedding_x_train = self.positional_encoding(self.token_embedding_x_train(x_train))\n",
    "        memory = self.transformer_encoder(embedding_x_train, mask_x_train)\n",
    "        embedding_y_data = self.positional_encoding(self.token_embedding_y_data(y_data))\n",
    "        outs = self.transformer_decoder(embedding_y_data, memory, mask_y_data)\n",
    "        output = self.output(outs)\n",
    "        return output\n",
    "\n",
    "    def encode(self, x_train, mask_x_train):\n",
    "        return self.transformer_encoder(self.positional_encoding(self.token_embedding_x_train(x_train)), mask_x_train)\n",
    "\n",
    "    def decode(self, y_data, memory, mask_y_data):\n",
    "        return self.transformer_decoder(self.positional_encoding(self.token_embedding_y_data(y_data)), memory, mask_y_data)\n",
    "\n",
    "# マスクの定義\n",
    "def create_mask(x_train, y_data):\n",
    "    seq_len_x_train = x_train.shape[1]\n",
    "    seq_len_y_data = y_data.shape[1]\n",
    "    mask_y_data = generate_square_subsequent_mask(seq_len_y_data).to(device)\n",
    "    mask_x_train = generate_square_subsequent_mask(seq_len_x_train).to(device)\n",
    "    return mask_x_train, mask_y_data\n",
    "\n",
    "def generate_square_subsequent_mask(seq_len):\n",
    "    mask = torch.triu(torch.full((seq_len, seq_len), float('-inf')), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "# 訓練、評価の処理を定義\n",
    "def train(model, data_provider, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    for x_train, y_data in data_provider:\n",
    "        x_train = x_train.float().to(device)\n",
    "        y_data = y_data.float().to(device)\n",
    "        \n",
    "        # 次元のチェックと拡張\n",
    "        if x_train.dim() == 2:\n",
    "            x_train = x_train.unsqueeze(1)\n",
    "        if y_data.dim() == 2:\n",
    "            y_data = y_data.unsqueeze(1)\n",
    "        \n",
    "        input_y_data = torch.cat((x_train[:, -1:, :], y_data[:, :-1, :]), dim=1)\n",
    "        mask_x_train, mask_y_data = create_mask(x_train, input_y_data)\n",
    "        output = model(x_train=x_train, y_data=input_y_data, mask_x_train=mask_x_train, mask_y_data=mask_y_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, y_data)\n",
    "        loss.backward()\n",
    "        total_loss.append(loss.cpu().detach())\n",
    "        optimizer.step()\n",
    "    return np.average(total_loss)\n",
    "\n",
    "def evaluate(flag, model, data_provider, criterion):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for x_train, y_data in data_provider:\n",
    "        x_train = x_train.float().to(device)\n",
    "        y_data = y_data.float().to(device)\n",
    "        \n",
    "        # 次元のチェックと拡張\n",
    "        if x_train.dim() == 2:\n",
    "            x_train = x_train.unsqueeze(1)\n",
    "        if y_data.dim() == 2:\n",
    "            y_data = y_data.unsqueeze(1)\n",
    "\n",
    "        seq_len_x_train = x_train.shape[1]\n",
    "        mask_x_train = (torch.zeros(seq_len_x_train, seq_len_x_train)).type(torch.bool)\n",
    "        mask_x_train = mask_x_train.float().to(device)\n",
    "        memory = model.encode(x_train, mask_x_train)\n",
    "        outputs = x_train[:, -1:, :]\n",
    "        seq_len_y_data = y_data.shape[1]\n",
    "        for i in range(seq_len_y_data - 1):\n",
    "            mask_y_data = (generate_square_subsequent_mask(outputs.size(1))).to(device)\n",
    "            output = model.decode(outputs, memory, mask_y_data)\n",
    "            output = model.output(output)\n",
    "            outputs = torch.cat([outputs, output[:, -1:, :]], dim=1)\n",
    "        loss = criterion(outputs, y_data)\n",
    "        total_loss.append(loss.cpu().detach())\n",
    "        all_true.append(torch.cat((x_train, y_data), dim=1).cpu().detach().numpy())\n",
    "        all_pred.append(torch.cat((x_train, outputs), dim=1).cpu().detach().numpy())\n",
    "    if flag == 'test':\n",
    "        true = np.concatenate(all_true)\n",
    "        pred = np.concatenate(all_pred)\n",
    "        df_true = pd.DataFrame(true.reshape(-1, 3), columns=['8_x', '8_y', '8_z'])\n",
    "        df_pred = pd.DataFrame(pred.reshape(-1, 3), columns=['8_x', '8_y', '8_z'])\n",
    "        df_true.to_csv('true_coordinates.csv', index=False)\n",
    "        df_pred.to_csv('predicted_coordinates.csv', index=False)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df_true['8_x'], label='true_x')\n",
    "        plt.plot(df_pred['8_x'], label='pred_x')\n",
    "        plt.plot(df_true['8_y'], label='true_y')\n",
    "        plt.plot(df_pred['8_y'], label='pred_y')\n",
    "        plt.plot(df_true['8_z'], label='true_z')\n",
    "        plt.plot(df_pred['8_z'], label='pred_z')\n",
    "        plt.legend()\n",
    "        plt.savefig('test_coordinates.pdf')\n",
    "    return np.average(total_loss)\n",
    "\n",
    "# パラメータなどの定義\n",
    "d_input = 3\n",
    "d_output = 3\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "dropout = 0.01\n",
    "x_train_len = 36\n",
    "y_data_len = 12\n",
    "batch_size = 1\n",
    "epochs = 3\n",
    "best_loss = float('Inf')\n",
    "best_model = None\n",
    "\n",
    "n_seq = 1\n",
    "num_joints = 21\n",
    "input_size = num_joints * 3\n",
    "hidden_size = 16\n",
    "output_size = num_joints * 3\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "print(\"モデルのトレーニングと評価を開始します\")\n",
    "\n",
    "#train_csv_path = 'hand_300.csv'\n",
    "test_csv_path = 'test_10/choki_test_10/gu_test.csv'\n",
    "test_csv_path = 'test_10/choki_test_10/choki_test.csv'\n",
    "\n",
    "x_train, y_train = preprocess_data(train_csv_path, n_seq, num_joints)\n",
    "x_test, y_test = preprocess_data(test_csv_path, n_seq, num_joints)\n",
    "\n",
    "train_loader = create_dataloader(x_train, y_train, batch_size)\n",
    "\n",
    "model = Transformer(num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, d_model=d_model, d_input=num_joints * 3, d_output=num_joints * 3, dim_feedforward=dim_feedforward, dropout=dropout, nhead=nhead)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=0.0001)\n",
    "\n",
    "valid_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_train = train(model=model, data_provider=train_loader, optimizer=optimizer, criterion=criterion)\n",
    "    loss_valid = evaluate(flag='val', model=model, data_provider=create_dataloader(x_test, y_test, batch_size), criterion=criterion)\n",
    "    if epoch % 10 == 0:\n",
    "        print('[{}/{}] train loss: {:.2f}, valid loss: {:.2f}'.format(epoch, epochs, loss_train, loss_valid))\n",
    "    valid_losses.append(loss_valid)\n",
    "    if best_loss > loss_valid:\n",
    "        best_loss = loss_valid\n",
    "        best_model = model\n",
    "\n",
    "print(\"モデルのテストを開始します\")\n",
    "evaluate(flag='test', model=best_model, data_provider=create_dataloader(x_test, y_test, batch_size), criterion=criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer_TimeSeriesForecasting-main-LRDpDuFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
