{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "x_test_batch: tensor([[[ 7.6413e-01,  3.6932e-01, -3.7836e-07,  8.1899e-01,  3.0497e-01,\n",
      "          -7.1325e-03,  8.5715e-01,  2.2283e-01, -7.8930e-03,  8.6296e-01,\n",
      "           1.3827e-01, -1.1969e-02,  8.3408e-01,  1.0765e-01, -1.4906e-02,\n",
      "           8.1962e-01,  1.6203e-01,  1.3213e-02,  8.3083e-01,  9.6545e-02,\n",
      "          -4.6149e-03,  8.3323e-01,  1.5066e-01, -1.6065e-02,  8.2725e-01,\n",
      "           1.8658e-01, -2.0687e-02,  7.9138e-01,  1.6256e-01,  8.7402e-03,\n",
      "           8.0524e-01,  1.0330e-01, -1.2801e-02,  8.0632e-01,  1.8114e-01,\n",
      "          -2.0404e-02,  7.9723e-01,  2.1832e-01, -1.9753e-02,  7.6224e-01,\n",
      "           1.6970e-01,  6.5971e-04,  7.7757e-01,  1.1086e-01, -2.4442e-02,\n",
      "           7.8060e-01,  1.8887e-01, -2.1026e-02,  7.7201e-01,  2.2944e-01,\n",
      "          -1.0974e-02,  7.2849e-01,  1.8369e-01, -7.7348e-03,  7.4609e-01,\n",
      "           1.2557e-01, -2.1765e-02,  7.5483e-01,  1.7525e-01, -1.3543e-02,\n",
      "           7.5076e-01,  2.1099e-01, -3.0234e-03,  7.4980e-01,  3.7677e-01,\n",
      "          -3.7755e-07,  8.0436e-01,  3.0784e-01, -6.0755e-03,  8.3943e-01,\n",
      "           2.2588e-01, -5.7510e-03,  8.4156e-01,  1.4552e-01, -8.8897e-03,\n",
      "           8.1565e-01,  1.1116e-01, -1.1055e-02,  8.0763e-01,  1.7124e-01,\n",
      "           1.6333e-02,  8.1270e-01,  1.0477e-01, -7.4178e-04,  8.1416e-01,\n",
      "           1.5488e-01, -1.3002e-02,  8.1054e-01,  1.9494e-01, -1.7982e-02,\n",
      "           7.7771e-01,  1.7243e-01,  1.2284e-02,  7.8528e-01,  1.1116e-01,\n",
      "          -7.7106e-03,  7.8662e-01,  1.8491e-01, -1.6069e-02,  7.8105e-01,\n",
      "           2.2801e-01, -1.5829e-02,  7.4761e-01,  1.8028e-01,  4.6164e-03,\n",
      "           7.5732e-01,  1.2354e-01, -1.7810e-02,  7.6058e-01,  1.9680e-01,\n",
      "          -1.5126e-02,  7.5575e-01,  2.4089e-01, -5.8893e-03,  7.1454e-01,\n",
      "           1.9480e-01, -3.4156e-03,  7.2584e-01,  1.3743e-01, -1.5106e-02,\n",
      "           7.3363e-01,  1.8407e-01, -7.1125e-03,  7.3213e-01,  2.2066e-01,\n",
      "           3.0480e-03,  7.2461e-01,  3.9490e-01, -3.3541e-07,  7.7800e-01,\n",
      "           3.2383e-01, -7.6146e-03,  8.1152e-01,  2.3807e-01, -7.7280e-03,\n",
      "           8.1342e-01,  1.5497e-01, -1.0735e-02,  7.8583e-01,  1.2589e-01,\n",
      "          -1.2700e-02,  7.7537e-01,  1.8945e-01,  1.2498e-02,  7.7726e-01,\n",
      "           1.2207e-01, -5.4824e-03,  7.8126e-01,  1.7014e-01, -1.8229e-02,\n",
      "           7.7970e-01,  2.1168e-01, -2.3251e-02,  7.4612e-01,  1.9453e-01,\n",
      "           9.3721e-03,  7.5030e-01,  1.3270e-01, -1.1704e-02,  7.5536e-01,\n",
      "           2.0319e-01, -2.0758e-02,  7.5238e-01,  2.4900e-01, -2.0867e-02,\n",
      "           7.1673e-01,  2.0605e-01,  2.6887e-03,  7.2225e-01,  1.4926e-01,\n",
      "          -1.9603e-02,  7.2930e-01,  2.1644e-01, -1.8258e-02,  7.2756e-01,\n",
      "           2.6253e-01, -1.0038e-02,  6.8407e-01,  2.2403e-01, -4.5626e-03,\n",
      "           6.9074e-01,  1.6811e-01, -1.6421e-02,  7.0185e-01,  2.0631e-01,\n",
      "          -1.0373e-02,  7.0401e-01,  2.4228e-01, -1.6492e-03]],\n",
      "\n",
      "        [[ 7.4980e-01,  3.7677e-01, -3.7755e-07,  8.0436e-01,  3.0784e-01,\n",
      "          -6.0755e-03,  8.3943e-01,  2.2588e-01, -5.7510e-03,  8.4156e-01,\n",
      "           1.4552e-01, -8.8897e-03,  8.1565e-01,  1.1116e-01, -1.1055e-02,\n",
      "           8.0763e-01,  1.7124e-01,  1.6333e-02,  8.1270e-01,  1.0477e-01,\n",
      "          -7.4178e-04,  8.1416e-01,  1.5488e-01, -1.3002e-02,  8.1054e-01,\n",
      "           1.9494e-01, -1.7982e-02,  7.7771e-01,  1.7243e-01,  1.2284e-02,\n",
      "           7.8528e-01,  1.1116e-01, -7.7106e-03,  7.8662e-01,  1.8491e-01,\n",
      "          -1.6069e-02,  7.8105e-01,  2.2801e-01, -1.5829e-02,  7.4761e-01,\n",
      "           1.8028e-01,  4.6164e-03,  7.5732e-01,  1.2354e-01, -1.7810e-02,\n",
      "           7.6058e-01,  1.9680e-01, -1.5126e-02,  7.5575e-01,  2.4089e-01,\n",
      "          -5.8893e-03,  7.1454e-01,  1.9480e-01, -3.4156e-03,  7.2584e-01,\n",
      "           1.3743e-01, -1.5106e-02,  7.3363e-01,  1.8407e-01, -7.1125e-03,\n",
      "           7.3213e-01,  2.2066e-01,  3.0480e-03,  7.2461e-01,  3.9490e-01,\n",
      "          -3.3541e-07,  7.7800e-01,  3.2383e-01, -7.6146e-03,  8.1152e-01,\n",
      "           2.3807e-01, -7.7280e-03,  8.1342e-01,  1.5497e-01, -1.0735e-02,\n",
      "           7.8583e-01,  1.2589e-01, -1.2700e-02,  7.7537e-01,  1.8945e-01,\n",
      "           1.2498e-02,  7.7726e-01,  1.2207e-01, -5.4824e-03,  7.8126e-01,\n",
      "           1.7014e-01, -1.8229e-02,  7.7970e-01,  2.1168e-01, -2.3251e-02,\n",
      "           7.4612e-01,  1.9453e-01,  9.3721e-03,  7.5030e-01,  1.3270e-01,\n",
      "          -1.1704e-02,  7.5536e-01,  2.0319e-01, -2.0758e-02,  7.5238e-01,\n",
      "           2.4900e-01, -2.0867e-02,  7.1673e-01,  2.0605e-01,  2.6887e-03,\n",
      "           7.2225e-01,  1.4926e-01, -1.9603e-02,  7.2930e-01,  2.1644e-01,\n",
      "          -1.8258e-02,  7.2756e-01,  2.6253e-01, -1.0038e-02,  6.8407e-01,\n",
      "           2.2403e-01, -4.5626e-03,  6.9074e-01,  1.6811e-01, -1.6421e-02,\n",
      "           7.0185e-01,  2.0631e-01, -1.0373e-02,  7.0401e-01,  2.4228e-01,\n",
      "          -1.6492e-03,  6.9824e-01,  4.2739e-01, -3.9796e-07,  7.4553e-01,\n",
      "           3.4402e-01, -5.7592e-03,  7.6979e-01,  2.5255e-01, -6.3126e-03,\n",
      "           7.6811e-01,  1.7024e-01, -1.0336e-02,  7.4097e-01,  1.4228e-01,\n",
      "          -1.2744e-02,  7.3231e-01,  2.0000e-01,  1.0719e-02,  7.3122e-01,\n",
      "           1.2806e-01, -7.5584e-03,  7.3807e-01,  1.8253e-01, -1.9708e-02,\n",
      "           7.3820e-01,  2.2534e-01, -2.4460e-02,  7.0494e-01,  2.1119e-01,\n",
      "           6.3025e-03,  7.0673e-01,  1.5453e-01, -1.3022e-02,  7.1579e-01,\n",
      "           2.2360e-01, -2.0318e-02,  7.1408e-01,  2.6345e-01, -2.0735e-02,\n",
      "           6.7784e-01,  2.2847e-01, -1.4889e-03,  6.8224e-01,  1.7249e-01,\n",
      "          -2.1946e-02,  6.9292e-01,  2.3951e-01, -1.8091e-02,  6.9245e-01,\n",
      "           2.8294e-01, -9.5551e-03,  6.4819e-01,  2.5162e-01, -9.9159e-03,\n",
      "           6.5693e-01,  1.9278e-01, -2.0438e-02,  6.6756e-01,  2.3150e-01,\n",
      "          -1.2599e-02,  6.6738e-01,  2.6652e-01, -3.2826e-03]],\n",
      "\n",
      "        [[ 7.2461e-01,  3.9490e-01, -3.3541e-07,  7.7800e-01,  3.2383e-01,\n",
      "          -7.6146e-03,  8.1152e-01,  2.3807e-01, -7.7280e-03,  8.1342e-01,\n",
      "           1.5497e-01, -1.0735e-02,  7.8583e-01,  1.2589e-01, -1.2700e-02,\n",
      "           7.7537e-01,  1.8945e-01,  1.2498e-02,  7.7726e-01,  1.2207e-01,\n",
      "          -5.4824e-03,  7.8126e-01,  1.7014e-01, -1.8229e-02,  7.7970e-01,\n",
      "           2.1168e-01, -2.3251e-02,  7.4612e-01,  1.9453e-01,  9.3721e-03,\n",
      "           7.5030e-01,  1.3270e-01, -1.1704e-02,  7.5536e-01,  2.0319e-01,\n",
      "          -2.0758e-02,  7.5238e-01,  2.4900e-01, -2.0867e-02,  7.1673e-01,\n",
      "           2.0605e-01,  2.6887e-03,  7.2225e-01,  1.4926e-01, -1.9603e-02,\n",
      "           7.2930e-01,  2.1644e-01, -1.8258e-02,  7.2756e-01,  2.6253e-01,\n",
      "          -1.0038e-02,  6.8407e-01,  2.2403e-01, -4.5626e-03,  6.9074e-01,\n",
      "           1.6811e-01, -1.6421e-02,  7.0185e-01,  2.0631e-01, -1.0373e-02,\n",
      "           7.0401e-01,  2.4228e-01, -1.6492e-03,  6.9824e-01,  4.2739e-01,\n",
      "          -3.9796e-07,  7.4553e-01,  3.4402e-01, -5.7592e-03,  7.6979e-01,\n",
      "           2.5255e-01, -6.3126e-03,  7.6811e-01,  1.7024e-01, -1.0336e-02,\n",
      "           7.4097e-01,  1.4228e-01, -1.2744e-02,  7.3231e-01,  2.0000e-01,\n",
      "           1.0719e-02,  7.3122e-01,  1.2806e-01, -7.5584e-03,  7.3807e-01,\n",
      "           1.8253e-01, -1.9708e-02,  7.3820e-01,  2.2534e-01, -2.4460e-02,\n",
      "           7.0494e-01,  2.1119e-01,  6.3025e-03,  7.0673e-01,  1.5453e-01,\n",
      "          -1.3022e-02,  7.1579e-01,  2.2360e-01, -2.0318e-02,  7.1408e-01,\n",
      "           2.6345e-01, -2.0735e-02,  6.7784e-01,  2.2847e-01, -1.4889e-03,\n",
      "           6.8224e-01,  1.7249e-01, -2.1946e-02,  6.9292e-01,  2.3951e-01,\n",
      "          -1.8091e-02,  6.9245e-01,  2.8294e-01, -9.5551e-03,  6.4819e-01,\n",
      "           2.5162e-01, -9.9159e-03,  6.5693e-01,  1.9278e-01, -2.0438e-02,\n",
      "           6.6756e-01,  2.3150e-01, -1.2599e-02,  6.6738e-01,  2.6652e-01,\n",
      "          -3.2826e-03,  6.6243e-01,  4.5550e-01, -3.3395e-07,  7.0421e-01,\n",
      "           3.6509e-01, -5.0574e-03,  7.2354e-01,  2.7080e-01, -5.9163e-03,\n",
      "           7.1632e-01,  1.9117e-01, -1.0341e-02,  6.8897e-01,  1.6231e-01,\n",
      "          -1.3410e-02,  6.8257e-01,  2.2272e-01,  1.0250e-02,  6.7743e-01,\n",
      "           1.5174e-01, -8.6389e-03,  6.8911e-01,  2.0232e-01, -2.2170e-02,\n",
      "           6.9300e-01,  2.4922e-01, -2.7889e-02,  6.5680e-01,  2.4133e-01,\n",
      "           4.7321e-03,  6.5239e-01,  1.7740e-01, -1.6061e-02,  6.6785e-01,\n",
      "           2.4044e-01, -2.5860e-02,  6.7122e-01,  2.8633e-01, -2.7599e-02,\n",
      "           6.3140e-01,  2.6722e-01, -3.9139e-03,  6.3207e-01,  2.1233e-01,\n",
      "          -2.5503e-02,  6.4928e-01,  2.7190e-01, -2.3964e-02,  6.5375e-01,\n",
      "           3.1689e-01, -1.6641e-02,  6.0348e-01,  2.9946e-01, -1.3123e-02,\n",
      "           6.0477e-01,  2.4106e-01, -2.5356e-02,  6.2124e-01,  2.6965e-01,\n",
      "          -1.9580e-02,  6.2796e-01,  3.0158e-01, -1.1547e-02]]])\n",
      "predicted_x: [[ 0.07862358  0.3408284  -0.54871166  0.8680585  -0.48231906 -0.42963222\n",
      "   0.1199096   0.01563619  0.09897759  0.6025506   1.5700736  -0.9173991\n",
      "  -0.3528756   0.11566667  0.17181243  0.24116729  0.36254317 -0.08958174\n",
      "   0.245355    0.03927818  0.8208222 ]\n",
      " [ 0.07482234  0.33876386 -0.5478796   0.8673665  -0.48453403 -0.42928544\n",
      "   0.12310066  0.01407093  0.09797517  0.60584354  1.573544   -0.9132968\n",
      "  -0.35236526  0.11561336  0.17474972  0.23327018  0.35987088 -0.08884768\n",
      "   0.24674024  0.04343947  0.82127434]\n",
      " [ 0.07101386  0.33550608 -0.5476086   0.86665136 -0.48710656 -0.42894202\n",
      "   0.12672389  0.01212534  0.09725543  0.60915226  1.5771391  -0.9089526\n",
      "  -0.35159022  0.11530686  0.17776534  0.22430556  0.35853586 -0.08785005\n",
      "   0.24783239  0.04860709  0.8233752 ]]\n",
      "MSE Loss(corrected): 0.5004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iwata/.local/share/virtualenvs/Transformer_TimeSeriesForecasting-main-LRDpDuFD/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 244\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE Loss(corrected): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# プロットの設定\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/iwata/Pictures/hand_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_n\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_seq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_w\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    245\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m#ax.set_aspect('equal', 'box')\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m#print(\"ground_truth_landmarks:\", ground_truth_landmarks)\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m#print(\"sample_landmarks_x:\",sample_landmarks_x)\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# 手の座標点の順序を指定するリスト（例）\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from torch.nn import TransformerEncoderLayer, LayerNorm, TransformerEncoder, TransformerDecoderLayer, TransformerDecoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import csv\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# フラットな手の座標をMediaPipe Handランドマークに変換する関数\n",
    "def flatten_to_landmarks(coordinates):\n",
    "    landmarks = []\n",
    "    for i in range(0, len(coordinates), 3):\n",
    "        landmarks.append((coordinates[i], coordinates[i + 1], coordinates[i + 2]))\n",
    "    return landmarks\n",
    "\n",
    "# データの読み込みと前処理\n",
    "def preprocess_data(csv_path, n_seq, num_joints):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for i in range(len(df) - n_seq):\n",
    "        x_sequence = df.iloc[i:i+n_seq][[f'{j}_{c}' for j in range(num_joints) for c in ['x', 'y', 'z']]].values.flatten()\n",
    "        y_sequence = df.iloc[i+n_seq][[f'{j}_x' for j in range(num_joints)]].values.flatten()\n",
    "        x_data.append(x_sequence)\n",
    "        y_data.append(y_sequence)\n",
    "\n",
    "    x_data = np.array(x_data, dtype=np.float32)\n",
    "    y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "# DataLoaderの使用\n",
    "def create_dataloader(x_train, y_train, batch_size):\n",
    "    train_dataset = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "# モデルのパラメータ\n",
    "n_seq = 3\n",
    "num_joints = 21\n",
    "input_size = num_joints * 3 * n_seq  # 各関節の座標 (x, y, z) を持つ\n",
    "hidden_size = 63\n",
    "output_size = num_joints  # すべての関節の x 座標を予測\n",
    "num_layers = 4\n",
    "batch_size = 36\n",
    "n_epochs = 100\n",
    "n_heads = 3\n",
    "\n",
    "# 位置エンコーディングの定義\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[:pe[:, 1::2].size(1)])  # Adjust the size for odd d_model\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# モデルに入力するために次元を拡張する\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Linear(c_in, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x)\n",
    "        return x\n",
    "\n",
    "# Transformerモデルのアーキテクチャ\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, n_heads):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = TokenEmbedding(input_size, hidden_size)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(hidden_size, n_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(hidden_size, n_heads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        memory = self.transformer_encoder(x)\n",
    "        #transformer_output = self.transformer_decoder(x, memory)\n",
    "        #out = self.fc(transformer_output)\n",
    "        out= self.transformer_decoder(x, memory)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# モデル、損失関数、オプティマイザ\n",
    "model = TransformerModel(input_size, hidden_size, output_size, num_layers, n_heads).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "# テストデータで評価\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    #test_outputs = model(torch.tensor(x_test, dtype=torch.float32).unsqueeze(1).to(device))\n",
    "    #test_loss = criterion(test_outputs, torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "    test_outputs = model(torch.tensor(x_test, dtype=torch.float32).unsqueeze(1).to(device))\n",
    "    test_loss = criterion(test_outputs, torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "    processing_time_per_image = time.time() - start_time\n",
    "    #print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    #print(f'処理時間_1: {processing_time_per_image:.6f} 秒')\n",
    "\n",
    "\n",
    "# メインの処理\n",
    "if __name__ == \"__main__\":\n",
    "    # データの読み込みと前処理\n",
    "    train_csv_path = 'test_10/choki_test_10/choki_test.csv'\n",
    "    test_csv_path = 'test_10/choki_test_10/choki_test.csv'\n",
    "    \n",
    "    x_train, y_train = preprocess_data(train_csv_path, n_seq, num_joints)\n",
    "    x_test, y_test = preprocess_data(test_csv_path, n_seq, num_joints)\n",
    "\n",
    "    # DataLoaderの作成\n",
    "    train_loader = create_dataloader(x_train, y_train, batch_size)\n",
    "\n",
    "    #  3つのテストサンプルごとに予測結果を処理し、1つずつずらして繰り返す\n",
    "    window_size = 3  # ウィンドウサイズ（処理するテストサンプルの数）\n",
    "\n",
    "    for sample_index in range(0, len(x_test)):  # ウィンドウを1つずつずらして処理する\n",
    "    #for sample_index in range(0, len(x_test) - window_size + 1):  # ウィンドウを1つずつずらして処理する\n",
    "        # ウィンドウ内のテストサンプルを取得\n",
    "        x_test_batch = torch.tensor(x_test[sample_index:sample_index+window_size], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        y_test_batch = torch.tensor(y_test[sample_index:sample_index+window_size], dtype=torch.float32).to(device)\n",
    "        print(sample_index+1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 既存のモデルでx_testを予測\n",
    "            start_time = time.time()\n",
    "            #\n",
    "            predicted_tensor_x = model(x_test_batch)\n",
    "            print(\"x_test_batch:\", x_test_batch)\n",
    "            predicted_x = predicted_tensor_x.cpu().numpy()\n",
    "            processing_time_per_image = time.time() - start_time\n",
    "            #print(f'処理時間_2: {processing_time_per_image:.6f} 秒')\n",
    "\n",
    "            #\"\"\"\n",
    "            # 新しいモデルのインスタンスを作成\n",
    "            new_model = TransformerModel(input_size, hidden_size, output_size, num_layers, n_heads).to(device)\n",
    "            #new_model.load_state_dict(torch.load('your_model.pth'))\n",
    "            #new_model.load_state_dict(torch.load('hanbetu_all.pth'))\n",
    "            new_model.to(device)\n",
    "            new_model.eval()\n",
    "            #\"\"\"\n",
    "\n",
    "            # 既存の手の座標を指定\n",
    "            # 予測結果の後に続く処理\n",
    "            #print(y_test_batch)\n",
    "            ground_truth_landmarks = flatten_to_landmarks(y_test_batch[0])  # 修正\n",
    "            #ground_truth_landmarks = flatten_to_landmarks(y_test[sample_index])\n",
    "            #print(y_test_batch)\n",
    "            np.savetxt('x_test.csv', x_test, delimiter=',', fmt='%f')\n",
    "            truth_landmarks = flatten_to_landmarks(x_test[sample_index])\n",
    "\n",
    "            ####################\n",
    "\n",
    "\n",
    "            print(\"predicted_x:\", predicted_x)\n",
    "            np.savetxt('predicter.csv', predicted_x, delimiter=',', fmt='%f')\n",
    "            sample_landmarks_x = flatten_to_landmarks(predicted_x[0])  # 予測結果を利用\n",
    "            \n",
    "            \n",
    "            \n",
    "            ##############\n",
    "\n",
    "\n",
    "\n",
    "            # landmark_0 の x 座標と y 座標の差分を計算し、表示する\n",
    "            x_difference = y_test[sample_index][0] - predicted_x[0][0]\n",
    "            y_difference = y_test[sample_index][1] - predicted_x[0][1]\n",
    "            #print(f'Landmark 0 の x 座標の差分: {x_difference}')\n",
    "            #print(f'Landmark 0 の y 座標の差分: {y_difference}')\n",
    "\n",
    "            # sample_landmarks_xに含まれるすべてのデータを修正してリストに変換\n",
    "            corrected = []\n",
    "            for landmark in sample_landmarks_x:\n",
    "                x_corrected = landmark[0] + x_difference\n",
    "                y_corrected = landmark[1] + y_difference\n",
    "                z_corrected = landmark[2]  # z座標は変更しないと仮定\n",
    "                corrected.append((x_corrected, y_corrected, z_corrected))\n",
    "    \n",
    "            # リストに変換\n",
    "            corrected = [list(landmark) for landmark in corrected]\n",
    "            #print(\"corrected:\", corrected)\n",
    "\n",
    "\n",
    "            # ground_truth_landmarksとsample_landmarks_xをTensorに変換\n",
    "            ground_truth_tensor = torch.tensor(ground_truth_landmarks, dtype=torch.float32)\n",
    "            sample_landmarks_x_tensor = torch.tensor(sample_landmarks_x, dtype=torch.float32)\n",
    "            # MSELossを計算\n",
    "            loss = criterion(ground_truth_tensor, sample_landmarks_x_tensor)\n",
    "            #print(f'MSE Loss(sample): {loss.item():.4f}')\n",
    "\n",
    "            \n",
    "            # ground_truth_landmarksとcorrectedをTensorに変換\n",
    "            ground_truth_tensor = torch.tensor(ground_truth_landmarks, dtype=torch.float32)\n",
    "            corrected_tensor = torch.tensor(corrected, dtype=torch.float32)\n",
    "            # MSELossを計算\n",
    "            loss = criterion(ground_truth_tensor, corrected_tensor)\n",
    "            print(f'MSE Loss(corrected): {loss.item():.4f}')\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "            # プロットの設定\n",
    "            save_path = f'/home/iwata/Pictures/hand_{sample_index}_n{n_seq}_r{i+1}_w{window_size}_{n_epochs}.png'\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "                #ax.set_aspect('equal', 'box')\n",
    "                #print(\"ground_truth_landmarks:\", ground_truth_landmarks)\n",
    "                #print(\"sample_landmarks_x:\",sample_landmarks_x)\n",
    "\n",
    "                # 手の座標点の順序を指定するリスト（例）\n",
    "            custom_order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "                # Ground Truthの手の線をプロット\n",
    "            for points in [[0, 1, 2, 3, 4], [0, 5, 6, 7, 8], [5, 9, 10, 11, 12], [9, 13, 14, 15, 16], [13, 17, 18, 19, 20], [0, 17]]:\n",
    "                x_points = [ground_truth_landmarks[i][0] for i in points]\n",
    "                y_points = [ground_truth_landmarks[i][1] for i in points]\n",
    "                ax.plot(x_points, y_points, linestyle='-', color='blue', linewidth=2)\n",
    "                '''\n",
    "                # sample_landmarks_xの手の線をプロット\n",
    "                for points in [[0, 1, 2, 3, 4], [0, 5, 6, 7, 8], [5, 9, 10, 11, 12], [9, 13, 14, 15, 16], [13, 17, 18, 19, 20], [0, 17]]:\n",
    "                    x_points = [sample_landmarks_x[i][0] for i in points]\n",
    "                    y_points = [sample_landmarks_x[i][1] for i in points]\n",
    "                    ax.plot(x_points, y_points, linestyle='-', color='red', linewidth=2)\n",
    "                '''\n",
    "                \n",
    "                # correctedの手の線をプロット\n",
    "            for points in [[0, 1, 2, 3, 4], [0, 5, 6, 7, 8], [5, 9, 10, 11, 12], [9, 13, 14, 15, 16], [13, 17, 18, 19, 20], [0, 17]]:\n",
    "                x_points = [corrected[i][0] for i in points]\n",
    "                y_points = [corrected[i][1] for i in points]\n",
    "                ax.plot(x_points, y_points, linestyle='-', color='red', linewidth=2)\n",
    "                    \n",
    "                # truth_landmarksの手の線をプロット\n",
    "            for points in [[0, 1, 2, 3, 4], [0, 5, 6, 7, 8], [5, 9, 10, 11, 12], [9, 13, 14, 15, 16], [13, 17, 18, 19, 20], [0, 17]]:\n",
    "                x_points = [truth_landmarks[i][0] for i in points]\n",
    "                y_points = [truth_landmarks[i][1] for i in points]\n",
    "                ax.plot(x_points, y_points, linestyle='-', color='green', linewidth=2)\n",
    "                \n",
    "        \n",
    "                #plt.legend()\n",
    "\n",
    "                # 画像を保存\n",
    "            plt.savefig(save_path)\n",
    "\n",
    "                # 画像を表示\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "    # トレーニングループ\n",
    "    start_time = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.unsqueeze(1).to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)  # デコーダのターゲットを入力と同じに設定\n",
    "            loss = criterion(outputs.squeeze(1), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # テストデータで評価\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        test_outputs = model(torch.tensor(x_test, dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        test_loss = criterion(test_outputs.squeeze(1), torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "        processing_time_per_image = time.time() - start_time\n",
    "\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    print(f'Training Time: {training_time:.4f} seconds')\n",
    "    print(f'Processing Time per Image: {processing_time_per_image} seconds')\n",
    "    \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer_TimeSeriesForecasting-main-LRDpDuFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
